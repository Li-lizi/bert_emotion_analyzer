"""
æƒ…æ„Ÿåˆ†æç³»ç»Ÿä¸»ç¨‹åº - é‡æ„ç‰ˆæœ¬
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›å®Œæ•´çš„å¾®åšè¯„è®ºæƒ…æ„Ÿåˆ†æåŠŸèƒ½
"""
import os
import sys
import json
import yaml
import pickle
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Any, Optional
import warnings

# å±è”½è­¦å‘Š
warnings.filterwarnings("ignore")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("outputs/logs/main.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.dictionary_manager import get_dictionary_manager
from utils.sentiment_enhancer import get_sentiment_enhancer
from utils.mental_health_analyzer import get_mental_health_analyzer
from utils.scene_classifier import get_scene_classifier
from utils.tfidf_analyzer import TFIDFAnalyzer
from utils.lda_topic_modeler import LDATopicModeler


class EmotionAnalyzer:
    """æƒ…æ„Ÿåˆ†æä¸»ç±»"""
    
    def __init__(self, config_path: str = "configs/paths_config.yaml"):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.models = {}
        self.modules = {}
        self.is_initialized = False
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("outputs/logs", exist_ok=True)
        os.makedirs("models", exist_ok=True)
        
        logger.info("æƒ…æ„Ÿåˆ†æå™¨åˆå§‹åŒ–ä¸­...")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                    return yaml.safe_load(f)
                elif config_path.endswith('.json'):
                    return json.load(f)
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                'bert_model': {
                    'path': 'models/bert_model',
                    'max_len': 128,
                    'batch_size': 32
                },
                'data': {
                    'train_csv': 'data/train.csv',
                    'val_csv': 'data/val.csv',
                    'test_csv': 'data/test.csv',
                    'text_col': 'cleaned_text',
                    'label_col': 'label'
                }
            }
    
    def initialize_modules(self) -> bool:
        """
        åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—
        
        Returns:
            åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—...")
            
            # 1. åˆå§‹åŒ–è¯å…¸ç®¡ç†å™¨
            logger.info("åˆå§‹åŒ–è¯å…¸ç®¡ç†å™¨...")
            self.modules['dictionary_manager'] = get_dictionary_manager()
            dict_manager = self.modules['dictionary_manager']
            dict_manager.load_all_dictionaries()
            
            # æ˜¾ç¤ºè¯å…¸ç»Ÿè®¡
            dict_stats = dict_manager.get_statistics()
            logger.info(f"è¯å…¸åŠ è½½å®Œæˆï¼Œå…± {dict_stats['total_dictionaries']} ç±»è¯å…¸ï¼Œ{dict_stats['total_keywords']} ä¸ªå…³é”®è¯")
            
            # 2. åˆå§‹åŒ–æƒ…æ„Ÿå¢å¼ºå™¨
            logger.info("åˆå§‹åŒ–æƒ…æ„Ÿå¢å¼ºå™¨...")
            self.modules['sentiment_enhancer'] = get_sentiment_enhancer()
            
            # 3. åˆå§‹åŒ–å¿ƒç†å¥åº·åˆ†æå™¨
            logger.info("åˆå§‹åŒ–å¿ƒç†å¥åº·åˆ†æå™¨...")
            self.modules['mental_health_analyzer'] = get_mental_health_analyzer()
            
            # 4. åˆå§‹åŒ–åœºæ™¯åˆ†ç±»å™¨
            logger.info("åˆå§‹åŒ–åœºæ™¯åˆ†ç±»å™¨...")
            self.modules['scene_classifier'] = get_scene_classifier()
            
            # 5. åˆå§‹åŒ–TF-IDFåˆ†æå™¨ï¼ˆä¸ç«‹å³è®­ç»ƒï¼‰
            logger.info("åˆå§‹åŒ–TF-IDFåˆ†æå™¨...")
            self.modules['tfidf_analyzer'] = TFIDFAnalyzer()
            
            # 6. åˆå§‹åŒ–LDAä¸»é¢˜å»ºæ¨¡å™¨ï¼ˆä¸ç«‹å³è®­ç»ƒï¼‰
            logger.info("åˆå§‹åŒ–LDAä¸»é¢˜å»ºæ¨¡å™¨...")
            self.modules['lda_modeler'] = LDATopicModeler()
            
            self.is_initialized = True
            logger.info("æ‰€æœ‰æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ¨¡å—å¤±è´¥: {e}")
            return False
    
    def load_models(self) -> bool:
        """
        åŠ è½½æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹
        
        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
            
            # 1. å°è¯•åŠ è½½TF-IDFæ¨¡å‹
            tfidf_analyzer = self.modules['tfidf_analyzer']
            tfidf_model_path = self.config.get('models', {}).get('tfidf', 'models/tfidf_model.pkl')
            
            if os.path.exists(tfidf_model_path):
                if tfidf_analyzer.load_model():
                    logger.info(f"TF-IDFæ¨¡å‹å·²åŠ è½½ï¼Œç‰¹å¾æ•°: {len(tfidf_analyzer.feature_names)}")
                    self.models['tfidf'] = tfidf_analyzer
                else:
                    logger.warning("TF-IDFæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†åœ¨éœ€è¦æ—¶è®­ç»ƒ")
            else:
                logger.info("TF-IDFæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åœ¨éœ€è¦æ—¶è®­ç»ƒ")
            
            # 2. å°è¯•åŠ è½½LDAæ¨¡å‹
            lda_modeler = self.modules['lda_modeler']
            lda_model_path = self.config.get('models', {}).get('lda', 'models/lda_model.pkl')
            
            if os.path.exists(lda_model_path):
                if lda_modeler.load_model():
                    logger.info(f"LDAæ¨¡å‹å·²åŠ è½½ï¼Œä¸»é¢˜æ•°: {lda_modeler.num_topics}")
                    self.models['lda'] = lda_modeler
                else:
                    logger.warning("LDAæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†åœ¨éœ€è¦æ—¶è®­ç»ƒ")
            else:
                logger.info("LDAæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åœ¨éœ€è¦æ—¶è®­ç»ƒ")
            
            # 3. è¿™é‡Œå¯ä»¥æ·»åŠ BERTæ¨¡å‹çš„åŠ è½½é€»è¾‘
            # æ³¨æ„ï¼šBERTæ¨¡å‹è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦å•ç‹¬å¤„ç†
            
            logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def train_tfidf_model(self, texts: List[str]) -> bool:
        """
        è®­ç»ƒTF-IDFæ¨¡å‹
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            
        Returns:
            è®­ç»ƒæ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹è®­ç»ƒTF-IDFæ¨¡å‹ï¼Œæ–‡æœ¬æ•°é‡: {len(texts)}")
            
            tfidf_analyzer = self.modules['tfidf_analyzer']
            success = tfidf_analyzer.train(texts)
            
            if success:
                # ä¿å­˜æ¨¡å‹
                tfidf_analyzer.save_model()
                self.models['tfidf'] = tfidf_analyzer
                logger.info("TF-IDFæ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜")
                return True
            else:
                logger.error("TF-IDFæ¨¡å‹è®­ç»ƒå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"è®­ç»ƒTF-IDFæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def train_lda_model(self, texts: List[str]) -> bool:
        """
        è®­ç»ƒLDAæ¨¡å‹
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            
        Returns:
            è®­ç»ƒæ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹è®­ç»ƒLDAæ¨¡å‹ï¼Œæ–‡æœ¬æ•°é‡: {len(texts)}")
            
            lda_modeler = self.modules['lda_modeler']
            success = lda_modeler.train(texts)
            
            if success:
                # ä¿å­˜æ¨¡å‹
                lda_modeler.save_model()
                self.models['lda'] = lda_modeler
                logger.info("LDAæ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜")
                return True
            else:
                logger.error("LDAæ¨¡å‹è®­ç»ƒå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"è®­ç»ƒLDAæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def analyze_single_text(self, text: str, use_bert: bool = True) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            use_bert: æ˜¯å¦ä½¿ç”¨BERTæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            
        Returns:
            å®Œæ•´çš„æƒ…æ„Ÿåˆ†æç»“æœ
        """
        if not self.is_initialized:
            logger.error("åˆ†æå™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize_modules()")
            return {'error': 'åˆ†æå™¨æœªåˆå§‹åŒ–'}
        
        try:
            logger.info(f"åˆ†ææ–‡æœ¬: {text[:50]}...")
            
            result = {
                'text': text,
                'length': len(text),
                'analyses': {}
            }
            
            # 1. åŸºç¡€BERTæƒ…æ„Ÿåˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é›†æˆæ‚¨åŸæ¥çš„BERTæ¨¡å‹
            bert_result = self._simulate_bert_analysis(text)
            result['analyses']['bert'] = bert_result
            
            # 2. æƒ…æ„Ÿå¢å¼ºåˆ†æ
            if 'sentiment_enhancer' in self.modules:
                enhancer = self.modules['sentiment_enhancer']
                enhanced_emotion, enhanced_confidence = enhancer.enhance_prediction(
                    text, bert_result['emotion'], bert_result['confidence']
                )
                
                result['analyses']['enhanced'] = {
                    'emotion': enhanced_emotion,
                    'confidence': enhanced_confidence,
                    'is_corrected': bert_result['emotion'] != enhanced_emotion
                }
            
            # 3. TF-IDFåˆ†æï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½ï¼‰
            if 'tfidf' in self.models:
                tfidf_result = self.models['tfidf'].analyze_sentiment_keywords(text)
                result['analyses']['tfidf'] = tfidf_result
            
            # 4. LDAä¸»é¢˜åˆ†æï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½ï¼‰
            if 'lda' in self.models:
                lda_result = self.models['lda'].analyze_text_topics(text)
                result['analyses']['lda'] = lda_result
            
            # 5. å¿ƒç†å¥åº·åˆ†æ
            if 'mental_health_analyzer' in self.modules:
                mental_result = self.modules['mental_health_analyzer'].get_detailed_analysis(text)
                result['analyses']['mental_health'] = mental_result
            
            # 6. åœºæ™¯åˆ†ç±»
            if 'scene_classifier' in self.modules:
                scene_result = self.modules['scene_classifier'].classify_with_details(text)
                result['analyses']['scene'] = scene_result
            
            # 7. ç»¼åˆå†³ç­–
            final_decision = self._make_final_decision(result['analyses'])
            result['final_decision'] = final_decision
            
            logger.info(f"åˆ†æå®Œæˆï¼Œæœ€ç»ˆæƒ…æ„Ÿ: {final_decision['emotion']} ({final_decision['confidence']:.3f})")
            return result
            
        except Exception as e:
            logger.error(f"åˆ†ææ–‡æœ¬å¤±è´¥: {e}")
            return {'error': str(e), 'text': text}
    
    def _simulate_bert_analysis(self, text: str) -> Dict[str, Any]:
        """
        æ¨¡æ‹ŸBERTæƒ…æ„Ÿåˆ†æï¼ˆå ä½å‡½æ•°ï¼‰
        åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨æ‚¨è®­ç»ƒå¥½çš„BERTæ¨¡å‹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ¨¡æ‹Ÿçš„åˆ†æç»“æœ
        """
        # è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå®ç°
        # å®é™…åº”ç”¨ä¸­åº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„BERTæ¨¡å‹è°ƒç”¨
        
        # ç®€å•è§„åˆ™ï¼šåŒ…å«è´Ÿé¢è¯åˆ™åˆ¤ä¸ºè´Ÿé¢ï¼Œå¦åˆ™åˆ¤ä¸ºæ­£é¢
        negative_keywords = ['éš¾è¿‡', 'æ‚²ä¼¤', 'ç—›è‹¦', 'å¤±æœ›', 'ç”Ÿæ°”', 'ç„¦è™‘', 'å‹åŠ›', 'ç´¯']
        positive_keywords = ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'æ»¡æ„', 'å–œæ¬¢', 'å¥½', 'æ£’']
        
        # ç»Ÿè®¡å…³é”®è¯
        neg_count = sum(1 for word in negative_keywords if word in text)
        pos_count = sum(1 for word in positive_keywords if word in text)
        
        if neg_count > pos_count:
            emotion = "è´Ÿé¢"
            confidence = min(0.95, 0.6 + neg_count * 0.05)
        elif pos_count > neg_count:
            emotion = "æ­£é¢"
            confidence = min(0.95, 0.6 + pos_count * 0.05)
        else:
            emotion = "ä¸­æ€§"
            confidence = 0.5
        
        return {
            'emotion': emotion,
            'confidence': confidence,
            'neg_count': neg_count,
            'pos_count': pos_count
        }
    
    def _make_final_decision(self, analyses: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç»¼åˆæ‰€æœ‰åˆ†æç»“æœåšå‡ºæœ€ç»ˆå†³ç­–
        
        Args:
            analyses: æ‰€æœ‰åˆ†æç»“æœ
            
        Returns:
            æœ€ç»ˆå†³ç­–ç»“æœ
        """
        # æ”¶é›†æ‰€æœ‰æƒ…æ„Ÿé¢„æµ‹
        emotions = []
        confidences = []
        
        # 1. BERTç»“æœ
        if 'bert' in analyses:
            emotions.append(analyses['bert']['emotion'])
            confidences.append(analyses['bert']['confidence'])
        
        # 2. å¢å¼ºåç»“æœï¼ˆå¦‚æœæœ‰ä¿®æ­£ï¼‰
        if 'enhanced' in analyses and analyses['enhanced']['is_corrected']:
            emotions.append(analyses['enhanced']['emotion'])
            confidences.append(analyses['enhanced']['confidence'])
        
        # 3. TF-IDFç»“æœ
        if 'tfidf' in analyses:
            tfidf_sentiment = analyses['tfidf'].get('sentiment', 'neutral')
            emotions.append('æ­£é¢' if tfidf_sentiment == 'positive' else 'è´Ÿé¢' if tfidf_sentiment == 'negative' else 'ä¸­æ€§')
            confidences.append(analyses['tfidf'].get('confidence', 0.5))
        
        # 4. LDAç»“æœ
        if 'lda' in analyses:
            lda_sentiment = analyses['lda'].get('topic_sentiment', 'neutral')
            emotions.append('æ­£é¢' if lda_sentiment == 'positive' else 'è´Ÿé¢' if lda_sentiment == 'negative' else 'ä¸­æ€§')
            confidences.append(0.7)  # LDAç½®ä¿¡åº¦è®¾ä¸ºå›ºå®šå€¼
        
        # æŠ•ç¥¨å†³ç­–
        if not emotions:
            return {'emotion': 'æœªçŸ¥', 'confidence': 0.0, 'decision_method': 'æ— ç»“æœ'}
        
        # ç»Ÿè®¡æƒ…æ„Ÿå‡ºç°æ¬¡æ•°
        from collections import Counter
        emotion_counts = Counter(emotions)
        
        # æ‰¾åˆ°æœ€é¢‘ç¹çš„æƒ…æ„Ÿ
        most_common = emotion_counts.most_common(1)[0]
        final_emotion = most_common[0]
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦ï¼ˆåªè€ƒè™‘åŒ¹é…æœ€ç»ˆæƒ…æ„Ÿçš„ç»“æœï¼‰
        matching_confidences = [conf for emo, conf in zip(emotions, confidences) if emo == final_emotion]
        if matching_confidences:
            avg_confidence = sum(matching_confidences) / len(matching_confidences)
        else:
            avg_confidence = 0.5
        
        return {
            'emotion': final_emotion,
            'confidence': avg_confidence,
            'decision_method': 'æŠ•ç¥¨èåˆ',
            'vote_count': len(emotions),
            'emotion_distribution': dict(emotion_counts)
        }
    
    def batch_analyze(self, texts: List[str], use_bert: bool = True) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†ææ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            use_bert: æ˜¯å¦ä½¿ç”¨BERTæ¨¡å‹
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æï¼Œå…± {len(texts)} æ¡æ–‡æœ¬")
        
        for i, text in enumerate(texts):
            try:
                result = self.analyze_single_text(text, use_bert)
                results.append(result)
                
                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 10 == 0 or i == len(texts) - 1:
                    logger.info(f"  å·²åˆ†æ {i + 1}/{len(texts)} æ¡æ–‡æœ¬")
                    
            except Exception as e:
                logger.error(f"åˆ†æç¬¬ {i + 1} æ¡æ–‡æœ¬å¤±è´¥: {e}")
                results.append({'error': str(e), 'text': text})
        
        logger.info(f"æ‰¹é‡åˆ†æå®Œæˆï¼ŒæˆåŠŸ {len([r for r in results if 'error' not in r])}/{len(texts)} æ¡")
        return results
    
    def save_results(self, results: List[Dict], output_path: str = "outputs/analysis_results.json"):
        """
        ä¿å­˜åˆ†æç»“æœ
        
        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ç®€åŒ–ç»“æœä»¥ä¾¿ä¿å­˜
            simplified_results = []
            for result in results:
                if 'error' in result:
                    simplified_results.append(result)
                    continue
                
                simplified = {
                    'text': result.get('text', ''),
                    'length': result.get('length', 0),
                    'final_decision': result.get('final_decision', {}),
                    'has_mental_health': 'mental_health' in result.get('analyses', {}),
                    'has_scene': 'scene' in result.get('analyses', {})
                }
                
                # æ·»åŠ ç®€è¦åˆ†æ
                if 'analyses' in result:
                    analyses = result['analyses']
                    if 'enhanced' in analyses:
                        simplified['emotion'] = analyses['enhanced']['emotion']
                        simplified['confidence'] = analyses['enhanced']['confidence']
                    elif 'bert' in analyses:
                        simplified['emotion'] = analyses['bert']['emotion']
                        simplified['confidence'] = analyses['bert']['confidence']
                    
                    if 'scene' in analyses:
                        simplified['scenes'] = [s['scene'] for s in analyses['scene'].get('scenes', [])][:2]
                    
                    if 'mental_health' in analyses:
                        simplified['mental_labels'] = list(analyses['mental_health'].get('mental_health_labels', {}).keys())
                
                simplified_results.append(simplified)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(simplified_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def generate_report(self, results: List[Dict]) -> Dict[str, Any]:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        if not results:
            return {'error': 'æ— åˆ†æç»“æœ'}
        
        # è¿‡æ»¤é”™è¯¯ç»“æœ
        valid_results = [r for r in results if 'error' not in r]
        
        if not valid_results:
            return {'error': 'æ— æœ‰æ•ˆåˆ†æç»“æœ'}
        
        report = {
            'total_texts': len(results),
            'valid_texts': len(valid_results),
            'error_rate': (len(results) - len(valid_results)) / len(results) if len(results) > 0 else 0,
            'emotion_distribution': {},
            'scene_distribution': {},
            'mental_health_distribution': {},
            'average_confidence': 0.0,
            'statistics': {}
        }
        
        # æƒ…æ„Ÿåˆ†å¸ƒ
        emotions = []
        confidences = []
        scenes = []
        mental_labels = []
        
        for result in valid_results:
            # æƒ…æ„Ÿç»Ÿè®¡
            if 'final_decision' in result:
                emotion = result['final_decision'].get('emotion', 'æœªçŸ¥')
                confidence = result['final_decision'].get('confidence', 0.0)
                emotions.append(emotion)
                confidences.append(confidence)
            
            # åœºæ™¯ç»Ÿè®¡
            if 'analyses' in result and 'scene' in result['analyses']:
                scene_result = result['analyses']['scene']
                for scene_info in scene_result.get('scenes', []):
                    scenes.append(scene_info.get('scene', 'æœªçŸ¥'))
            
            # å¿ƒç†å¥åº·æ ‡ç­¾ç»Ÿè®¡
            if 'analyses' in result and 'mental_health' in result['analyses']:
                mental_result = result['analyses']['mental_health']
                for label in mental_result.get('mental_health_labels', {}).keys():
                    mental_labels.append(label)
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†å¸ƒ
        from collections import Counter
        if emotions:
            emotion_counts = Counter(emotions)
            report['emotion_distribution'] = dict(emotion_counts)
            report['average_confidence'] = sum(confidences) / len(confidences) if confidences else 0.0
        
        # è®¡ç®—åœºæ™¯åˆ†å¸ƒ
        if scenes:
            scene_counts = Counter(scenes)
            report['scene_distribution'] = dict(scene_counts)
        
        # è®¡ç®—å¿ƒç†å¥åº·æ ‡ç­¾åˆ†å¸ƒ
        if mental_labels:
            mental_counts = Counter(mental_labels)
            report['mental_health_distribution'] = dict(mental_counts)
        
        # ç»Ÿè®¡æ•°æ®
        report['statistics'] = {
            'emotion_count': len(emotions),
            'scene_count': len(scenes),
            'mental_label_count': len(mental_labels),
            'most_common_emotion': max(report['emotion_distribution'].items(), key=lambda x: x[1])[0] if report['emotion_distribution'] else 'æ— ',
            'most_common_scene': max(report['scene_distribution'].items(), key=lambda x: x[1])[0] if report['scene_distribution'] else 'æ— ',
            'most_common_mental_label': max(report['mental_health_distribution'].items(), key=lambda x: x[1])[0] if report['mental_health_distribution'] else 'æ— '
        }
        
        return report
    
    def interactive_mode(self):
        """äº¤äº’å¼åˆ†ææ¨¡å¼"""
        print("\n" + "=" * 70)
        print("ğŸ­ å¾®åšè¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ - äº¤äº’æ¨¡å¼")
        print("=" * 70)
        print("åŠŸèƒ½è¯´æ˜:")
        print("  1. è¾“å…¥æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æ")
        print("  2. è¾“å…¥ 'batch' è¿›å…¥æ‰¹é‡åˆ†ææ¨¡å¼")
        print("  3. è¾“å…¥ 'train' è¿›å…¥æ¨¡å‹è®­ç»ƒæ¨¡å¼")
        print("  4. è¾“å…¥ 'report' æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
        print("  5. è¾“å…¥ 'quit' æˆ– 'é€€å‡º' ç»“æŸç¨‹åº")
        print("=" * 70)
        
        while True:
            try:
                user_input = input("\nğŸ“ è¯·è¾“å…¥æ–‡æœ¬æˆ–å‘½ä»¤: ").strip()
                
                if user_input.lower() in ['quit', 'é€€å‡º', 'exit', 'q']:
                    print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break
                
                elif user_input.lower() == 'batch':
                    self._batch_mode()
                
                elif user_input.lower() == 'train':
                    self._train_mode()
                
                elif user_input.lower() == 'report':
                    self._show_report()
                
                elif not user_input:
                    print("âš ï¸  è¯·è¾“å…¥æ–‡æœ¬æˆ–å‘½ä»¤")
                    continue
                
                else:
                    # åˆ†æå•æ¡æ–‡æœ¬
                    result = self.analyze_single_text(user_input)
                    
                    if 'error' in result:
                        print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
                        continue
                    
                    # æ˜¾ç¤ºç»“æœ
                    self._display_result(result)
                    
            except KeyboardInterrupt:
                print("\n\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    def _display_result(self, result: Dict[str, Any]):
        """æ˜¾ç¤ºåˆ†æç»“æœ"""
        print("\nğŸ“Š åˆ†æç»“æœ:")
        print("-" * 50)
        
        text = result.get('text', '')
        if len(text) > 60:
            display_text = text[:57] + "..."
        else:
            display_text = text
        
        print(f"æ–‡æœ¬: {display_text}")
        print(f"é•¿åº¦: {result.get('length', 0)} å­—ç¬¦")
        
        # æ˜¾ç¤ºæœ€ç»ˆå†³ç­–
        if 'final_decision' in result:
            decision = result['final_decision']
            emotion = decision.get('emotion', 'æœªçŸ¥')
            confidence = decision.get('confidence', 0.0)
            
            # æƒ…æ„Ÿé¢œè‰²æ ‡è®°
            if emotion == "æ­£é¢":
                emotion_display = f"âœ… {emotion}"
            elif emotion == "è´Ÿé¢":
                emotion_display = f"âŒ {emotion}"
            else:
                emotion_display = f"âšª {emotion}"
            
            print(f"æœ€ç»ˆæƒ…æ„Ÿ: {emotion_display} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            if 'decision_method' in decision:
                print(f"å†³ç­–æ–¹æ³•: {decision['decision_method']}")
        
        # æ˜¾ç¤ºåœºæ™¯åˆ†ç±»
        if 'analyses' in result and 'scene' in result['analyses']:
            scene_result = result['analyses']['scene']
            if scene_result.get('scenes'):
                scenes = [f"{s['scene']}({s['confidence']:.2f})" for s in scene_result['scenes'][:2]]
                print(f"åœºæ™¯è¯†åˆ«: {', '.join(scenes)}")
        
        # æ˜¾ç¤ºå¿ƒç†å¥åº·æ ‡ç­¾
        if 'analyses' in result and 'mental_health' in result['analyses']:
            mental_result = result['analyses']['mental_health']
            if mental_result.get('mental_health_labels'):
                labels = list(mental_result['mental_health_labels'].keys())
                print(f"å¿ƒç†å¥åº·: {', '.join(labels)}")
                
                if mental_result.get('recommendation'):
                    print(f"å»ºè®®: {mental_result['recommendation'][:50]}...")
        
        # æ˜¾ç¤ºæ˜¯å¦è¢«ä¿®æ­£
        if 'analyses' in result and 'enhanced' in result['analyses']:
            enhanced = result['analyses']['enhanced']
            if enhanced.get('is_corrected'):
                print(f"ğŸ“ æƒ…æ„Ÿå·²ä¿®æ­£ (BERT: {result['analyses'].get('bert', {}).get('emotion', 'æœªçŸ¥')} â†’ {enhanced.get('emotion', 'æœªçŸ¥')})")
        
        print("-" * 50)
    
    def _batch_mode(self):
        """æ‰¹é‡åˆ†ææ¨¡å¼"""
        print("\nğŸ“š æ‰¹é‡åˆ†ææ¨¡å¼")
        print("-" * 50)
        
        # ä»æ–‡ä»¶åŠ è½½æˆ–æ‰‹åŠ¨è¾“å…¥
        source = input("é€‰æ‹©æ•°æ®æ¥æº (1=æ–‡ä»¶, 2=æ‰‹åŠ¨è¾“å…¥, å…¶ä»–=è¿”å›): ").strip()
        
        texts = []
        
        if source == '1':
            # ä»æ–‡ä»¶åŠ è½½
            file_path = input("è¯·è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„ (æ¯è¡Œä¸€ä¸ªæ–‡æœ¬): ").strip()
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        texts = [line.strip() for line in f if line.strip()]
                    print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    return
            else:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return
        
        elif source == '2':
            # æ‰‹åŠ¨è¾“å…¥
            print("è¯·è¾“å…¥æ–‡æœ¬ (æ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸ):")
            while True:
                line = input().strip()
                if not line:
                    break
                texts.append(line)
            print(f"æ‰‹åŠ¨è¾“å…¥äº† {len(texts)} æ¡æ–‡æœ¬")
        
        else:
            print("è¿”å›ä¸»èœå•")
            return
        
        if not texts:
            print("æ²¡æœ‰æ–‡æœ¬å¯åˆ†æ")
            return
        
        # å¼€å§‹æ‰¹é‡åˆ†æ
        print(f"\nå¼€å§‹åˆ†æ {len(texts)} æ¡æ–‡æœ¬...")
        results = self.batch_analyze(texts)
        
        # ä¿å­˜ç»“æœ
        save_option = input("æ˜¯å¦ä¿å­˜ç»“æœ? (y/n): ").strip().lower()
        if save_option == 'y':
            output_path = input("è¯·è¾“å…¥ä¿å­˜è·¯å¾„ (é»˜è®¤: outputs/batch_results.json): ").strip()
            if not output_path:
                output_path = "outputs/batch_results.json"
            
            if self.save_results(results, output_path):
                print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(results)
            print("\nğŸ“ˆ æ‰¹é‡åˆ†ææŠ¥å‘Š:")
            print(f"  æ€»æ–‡æœ¬æ•°: {report.get('total_texts', 0)}")
            print(f"  æœ‰æ•ˆåˆ†æ: {report.get('valid_texts', 0)}")
            print(f"  é”™è¯¯ç‡: {report.get('error_rate', 0):.2%}")
            
            if 'emotion_distribution' in report:
                print(f"  æƒ…æ„Ÿåˆ†å¸ƒ: {report['emotion_distribution']}")
            
            if 'average_confidence' in report:
                print(f"  å¹³å‡ç½®ä¿¡åº¦: {report['average_confidence']:.3f}")
        
        print("\næ‰¹é‡åˆ†æå®Œæˆ")
    
    def _train_mode(self):
        """æ¨¡å‹è®­ç»ƒæ¨¡å¼"""
        print("\nğŸ¤– æ¨¡å‹è®­ç»ƒæ¨¡å¼")
        print("-" * 50)
        print("å¯è®­ç»ƒçš„æ¨¡å‹:")
        print("  1. TF-IDF æ¨¡å‹")
        print("  2. LDA ä¸»é¢˜æ¨¡å‹")
        print("  3. è¿”å›ä¸»èœå•")
        
        choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        if choice == '1':
            self._train_tfidf()
        elif choice == '2':
            self._train_lda()
        else:
            print("è¿”å›ä¸»èœå•")
            return
    
    def _train_tfidf(self):
        """è®­ç»ƒTF-IDFæ¨¡å‹"""
        print("\nğŸ”§ è®­ç»ƒTF-IDFæ¨¡å‹")
        
        # è·å–è®­ç»ƒæ•°æ®
        data_source = input("é€‰æ‹©æ•°æ®æ¥æº (1=ä½¿ç”¨å†…ç½®è®­ç»ƒæ•°æ®, 2=è‡ªå®šä¹‰æ–‡ä»¶): ").strip()
        
        texts = []
        
        if data_source == '1':
            # ä½¿ç”¨å†…ç½®è®­ç»ƒæ•°æ®
            train_csv = self.config.get('data', {}).get('train_csv', 'data/train.csv')
            if os.path.exists(train_csv):
                try:
                    df = pd.read_csv(train_csv, encoding='utf-8-sig')
                    text_col = self.config.get('data', {}).get('text_col', 'cleaned_text')
                    
                    if text_col in df.columns:
                        texts = df[text_col].dropna().tolist()
                        print(f"ä»è®­ç»ƒæ•°æ®åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
                    else:
                        print(f"åˆ— {text_col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
                        return
                except Exception as e:
                    print(f"è¯»å–è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
                    return
            else:
                print(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_csv}")
                return
        
        elif data_source == '2':
            # è‡ªå®šä¹‰æ–‡ä»¶
            file_path = input("è¯·è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„: ").strip()
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        texts = [line.strip() for line in f if line.strip()]
                    print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    return
            else:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return
        
        else:
            print("è¿”å›")
            return
        
        if len(texts) < 10:
            print(f"æ–‡æœ¬æ•°é‡ä¸è¶³ ({len(texts)} æ¡)ï¼Œè‡³å°‘éœ€è¦ 10 æ¡æ–‡æœ¬")
            return
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nå¼€å§‹è®­ç»ƒTF-IDFæ¨¡å‹ï¼Œä½¿ç”¨ {len(texts)} æ¡æ–‡æœ¬...")
        success = self.train_tfidf_model(texts)
        
        if success:
            print("âœ… TF-IDFæ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            if 'tfidf' in self.models:
                stats = self.models['tfidf'].get_statistics()
                print(f"  ç‰¹å¾æ•°: {stats.get('vocabulary_size', 'æœªçŸ¥')}")
                print(f"  æ˜¯å¦è®­ç»ƒå®Œæˆ: {'æ˜¯' if stats.get('is_trained', False) else 'å¦'}")
        else:
            print("âŒ TF-IDFæ¨¡å‹è®­ç»ƒå¤±è´¥")
    
    def _train_lda(self):
        """è®­ç»ƒLDAæ¨¡å‹"""
        print("\nğŸ¯ è®­ç»ƒLDAä¸»é¢˜æ¨¡å‹")
        
        # è·å–è®­ç»ƒæ•°æ®ï¼ˆä¸TF-IDFç±»ä¼¼ï¼‰
        train_csv = self.config.get('data', {}).get('train_csv', 'data/train.csv')
        
        if os.path.exists(train_csv):
            try:
                df = pd.read_csv(train_csv, encoding='utf-8-sig')
                text_col = self.config.get('data', {}).get('text_col', 'cleaned_text')
                
                if text_col in df.columns:
                    texts = df[text_col].dropna().tolist()
                    print(f"ä»è®­ç»ƒæ•°æ®åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
                else:
                    print(f"åˆ— {text_col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
                    return
            except Exception as e:
                print(f"è¯»å–è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
                return
        else:
            print(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_csv}")
            return
        
        if len(texts) < 20:
            print(f"æ–‡æœ¬æ•°é‡ä¸è¶³ ({len(texts)} æ¡)ï¼Œè‡³å°‘éœ€è¦ 20 æ¡æ–‡æœ¬")
            return
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nå¼€å§‹è®­ç»ƒLDAæ¨¡å‹ï¼Œä½¿ç”¨ {len(texts)} æ¡æ–‡æœ¬...")
        success = self.train_lda_model(texts)
        
        if success:
            print("âœ… LDAæ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            if 'lda' in self.models:
                stats = self.models['lda'].get_statistics()
                print(f"  ä¸»é¢˜æ•°: {stats.get('num_topics', 'æœªçŸ¥')}")
                print(f"  è¯æ±‡é‡: {stats.get('vocabulary_size', 'æœªçŸ¥')}")
                
                # æ˜¾ç¤ºä¸»é¢˜æƒ…æ„Ÿåˆ†å¸ƒ
                if 'topic_sentiment_distribution' in stats:
                    dist = stats['topic_sentiment_distribution']
                    print(f"  ä¸»é¢˜æƒ…æ„Ÿåˆ†å¸ƒ: æ­£é¢={dist.get('positive', 0)}, è´Ÿé¢={dist.get('negative', 0)}, ä¸­æ€§={dist.get('neutral', 0)}")
        else:
            print("âŒ LDAæ¨¡å‹è®­ç»ƒå¤±è´¥")
    
    def _show_report(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€æŠ¥å‘Š"""
        print("\nğŸ“ˆ ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š")
        print("-" * 50)
        
        # æ¨¡å—çŠ¶æ€
        print("æ¨¡å—çŠ¶æ€:")
        print(f"  è¯å…¸ç®¡ç†å™¨: {'âœ“ å·²åŠ è½½' if 'dictionary_manager' in self.modules else 'âœ— æœªåŠ è½½'}")
        print(f"  æƒ…æ„Ÿå¢å¼ºå™¨: {'âœ“ å·²åŠ è½½' if 'sentiment_enhancer' in self.modules else 'âœ— æœªåŠ è½½'}")
        print(f"  å¿ƒç†å¥åº·åˆ†æå™¨: {'âœ“ å·²åŠ è½½' if 'mental_health_analyzer' in self.modules else 'âœ— æœªåŠ è½½'}")
        print(f"  åœºæ™¯åˆ†ç±»å™¨: {'âœ“ å·²åŠ è½½' if 'scene_classifier' in self.modules else 'âœ— æœªåŠ è½½'}")
        print(f"  TF-IDFåˆ†æå™¨: {'âœ“ å·²åŠ è½½' if 'tfidf_analyzer' in self.modules else 'âœ— æœªåŠ è½½'}")
        print(f"  LDAä¸»é¢˜å»ºæ¨¡å™¨: {'âœ“ å·²åŠ è½½' if 'lda_modeler' in self.modules else 'âœ— æœªåŠ è½½'}")
        
        # æ¨¡å‹çŠ¶æ€
        print("\næ¨¡å‹çŠ¶æ€:")
        print(f"  TF-IDFæ¨¡å‹: {'âœ“ å·²åŠ è½½' if 'tfidf' in self.models else 'âœ— æœªè®­ç»ƒ/æœªåŠ è½½'}")
        print(f"  LDAæ¨¡å‹: {'âœ“ å·²åŠ è½½' if 'lda' in self.models else 'âœ— æœªè®­ç»ƒ/æœªåŠ è½½'}")
        print(f"  BERTæ¨¡å‹: {'âš ï¸  éœ€è¦å•ç‹¬é›†æˆ'}")
        
        # è¯å…¸ç»Ÿè®¡
        if 'dictionary_manager' in self.modules:
            dict_manager = self.modules['dictionary_manager']
            stats = dict_manager.get_statistics()
            print(f"\nè¯å…¸ç»Ÿè®¡:")
            print(f"  è¯å…¸æ€»æ•°: {stats.get('total_dictionaries', 0)}")
            print(f"  æ€»å…³é”®è¯æ•°: {stats.get('total_keywords', 0)}")
            
            # æ˜¾ç¤ºå„ç±»è¯å…¸æ•°é‡
            for key, count in stats.items():
                if key.startswith('sentiment_') or key.startswith('slang_') or key.startswith('mental_health_'):
                    dict_name = key.replace('_', ' ').title()
                    print(f"  {dict_name}: {count} è¯")
        
        print("\næç¤º: ä½¿ç”¨ 'train' å‘½ä»¤è®­ç»ƒç¼ºå¤±çš„æ¨¡å‹")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸ­ å¾®åšè¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
    print("ç‰ˆæœ¬: 2.0 (é‡æ„ç‰ˆ) | é›†æˆTF-IDF + LDA + å¤šç­–ç•¥å¢å¼º")
    print("=" * 70)
    
    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = EmotionAnalyzer()
        
        # åˆå§‹åŒ–æ¨¡å—
        print("åˆå§‹åŒ–ç³»ç»Ÿæ¨¡å—...")
        if not analyzer.initialize_modules():
            print("âŒ æ¨¡å—åˆå§‹åŒ–å¤±è´¥")
            return
        
        # åŠ è½½æ¨¡å‹
        print("åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        analyzer.load_models()
        
        print("âœ… ç³»ç»Ÿå‡†å¤‡å°±ç»ª")
        
        # è¿›å…¥äº¤äº’æ¨¡å¼
        analyzer.interactive_mode()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()